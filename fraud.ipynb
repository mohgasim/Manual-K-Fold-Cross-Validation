{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Creating a K-Fold Cross Validation Algorithm\n",
    "#### Optimizing Recall and Precision for Imbalaced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Mohammed Azhari Gasim (email: mohammedagasim@gmail.com)\n",
    "\n",
    "#### Introduction\n",
    "Running classification algorithms on a imbalanced dataset involves the challenge of achieving a high recall score while maintaining a reasonable precision score. Here I attempt to do this with a dataset containing credit card transactions, a small number of which are fraudulent. In the process I demonstrate how to manually create a K-Fold Cross Validation Algorithm that allows the user greater control of hyperparameters. \n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection.\n",
    "\n",
    "I will follow the process below:\n",
    "1. Running a cross validated Logistic Regression Classification algorithm on the raw data to establish baseline performance.\n",
    "2. Manually creating a 10-fold cross validated Logistic Regression algorithm.\n",
    "3. Using the manually created algorithm to adjust probability thresholds.\n",
    "4. Feature selection using coefficient p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset\n",
    "First I import the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('creditcard.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large dataset. It is made up of 31 columns and 284,807 samples of real credit card transactions. Most of the columns have, for privacy reasons, been anonymized. The last column is class, a value of 0 indicating a non-fraudulent transaction, and 1 indicating a non-fraudulent transaction. The class column is the target (dependent variable) and the others are the features (independent variables). Next I investigate the number of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    284315\n",
       " 1       492\n",
       " Name: Class, dtype: int64, 0.001727485630620034)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.Class.value_counts(), dataset.Class.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 284,807 transactions in the dataset, 284,315 (99.8%) are non-fraudulent and only 492 (0.2%) are fraudulent, meaning that this dataset is highly imbalanced.\n",
    "\n",
    "#### Establishing a Baseline\n",
    "To begin with, I will run a cross validated logistic regression classifier to establish baseline recall and precision scores. The accuracy score is irrelevant to this problem. Also note that, due to the highly imbalanced nature of the dataset, the null classifier (a classifier that simply assigns all samples to the most frequently occurring class) would have an accuracy score of 99.8%!\n",
    "\n",
    "I will use Sci-Kit Learn's LogisticRegerssion and cross_val_score packages. The logistic regression classifier is cross validated over 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall': 0.6085714285714285, 'Precision': 0.8403980762803129}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = np.array(dataset.iloc[:, dataset.columns != 'Class'])\n",
    "y = np.array(dataset.iloc[:, dataset.columns == 'Class']).reshape(len(X),)\n",
    "results = {}\n",
    "\n",
    "for score in ['Recall', 'Precision']:\n",
    "    logreg = LogisticRegression(solver = 'liblinear')\n",
    "    results[score] = cross_val_score(logreg, X, y, cv = 10, scoring = score.lower()).mean()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the baseline classifier are not good. The recall and precision scores are about 61% and 84%, respectively. This means that, of those sample classified as fraudulent, 84% of actually are, however, these themselves make up only 61% of all fraudulent transactions. Using this classifier in a real world application would mean potentially missing 39% of fraudulent credit card transactions.  \n",
    "\n",
    "The Logistic Regression classifier works by first calculating a probability that a given sample (feature vector) belongs to the fraudulent class. Next it assigns all samples whose probability is greater than a certain threshold to the positive class (in our case that is the fraudulent class). Sci-Kit Learn sets this threshold at 0.5.\n",
    "\n",
    "By decreasing this threshold we can improve the recall score of the model. However this comes at the cost of a lower level of precision. In short, we will correctly classify more of the fraudulent transactions, but end up with more non-fraudulent transactions being classified as fraudulent as well. The challenge here is to increase the recall score while maintaining a reasonable level of precision. We may get lucky however. It may be the case that we can increase the recall score without a large reduction in precision such that it remains reasonable.\n",
    "\n",
    "Unfortunately there is no way to adjust the probability threshold on Sci-Kit Learn while using the cross validation. To do so we must build our own folds and test each manually.\n",
    "\n",
    "#### Creating a K-Fold Cross Validation Algorithm Manually\n",
    "To create our own cross validation algorithm, we first create 10 folds using Ski-Kit Learn's StratifiedKFold package. The 'Stratified' in the name tells us that it maintains the class distributions in the target when creating the train and test splits. To test that the algorithm performs as expected, I first run it using a probability threshold of 0.5 in order to compare the results to the previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Recall': 0.6085714285714285, 'Precision': 0.8403980762803129}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10)\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "    \n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    logreg = LogisticRegression(solver = 'liblinear')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "    y_pred_class = [1 if p > 0.5 else 0 for p in y_pred_proba]\n",
    "    recall_list.append(metrics.recall_score(y_test, y_pred_class))\n",
    "    precision_list.append(metrics.precision_score(y_test, y_pred_class))\n",
    "results = {'Recall': np.mean(recall_list), 'Precision': np.mean(precision_list)}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our algorithm are exactly the same as the ones produced by the built in cross_val_score package. We can now confidently move on to adjusting the probability threshold.\n",
    "\n",
    "#### Adjusting Probability Threshold\n",
    "I now wrap my previous algorithm in a for loop to generate results for various probability thresholds (0.01 to 0.1, in increments of 0.01). I also save the recall and precision results for each threshold in recall_all_features and precision_all_features lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.01\n",
      "{'Recall': 0.8102448979591838, 'Precision': 0.38217657300567154}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.02\n",
      "{'Recall': 0.7755510204081633, 'Precision': 0.5216371629104887}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.03\n",
      "{'Recall': 0.7634285714285716, 'Precision': 0.5804849842726745}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.04\n",
      "{'Recall': 0.7532244897959184, 'Precision': 0.62344967815037}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.05\n",
      "{'Recall': 0.745061224489796, 'Precision': 0.6545254799148028}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.06\n",
      "{'Recall': 0.7348571428571429, 'Precision': 0.6691622537705219}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.07\n",
      "{'Recall': 0.7307755102040815, 'Precision': 0.6885769656552103}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.08\n",
      "{'Recall': 0.722612244897959, 'Precision': 0.7028937552220123}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.09\n",
      "{'Recall': 0.7165306122448979, 'Precision': 0.7116558783611363}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.1\n",
      "{'Recall': 0.7124489795918367, 'Precision': 0.7211867418646473}\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "recall_all_features = []\n",
    "precision_all_features = []\n",
    "\n",
    "for threshold in [i/100 for i in range(1, 11)]:\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        logreg = LogisticRegression(solver = 'liblinear')\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "        y_pred_class = [1 if p > threshold else 0 for p in y_pred_proba]\n",
    "        recall_list.append(metrics.recall_score(y_test, y_pred_class))\n",
    "        precision_list.append(metrics.precision_score(y_test, y_pred_class))\n",
    "    recall_all_features.append(np.mean(recall_list))\n",
    "    precision_all_features.append(np.mean(precision_list))\n",
    "    results = {'Recall': np.mean(recall_list), 'Precision': np.mean(precision_list)}\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print('Threshold: ' + str(threshold))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as expected. As the threshold decreases, the recall score increases and the precision score decreases. At a probability threshold of 0.01, the recall is improved, up from 61% to 81%, a 20 percentage point increase. However, at that level the precision has deteriorated significantly, down from 84% to 38%. In most real life situations, this would not be feasible. However the results do show some improvement. With a threshold of 0.08, the precision is improved, up 11 percentage points to 72%, while the precision level remains above 70%. \n",
    "\n",
    "We can attempt to improve these results by performing some feature selection.\n",
    "\n",
    "#### Feature Selection\n",
    "The model currently has 30 features. Some may contain more noise than signal, removing those may improve the model.\n",
    "\n",
    "To identify the most relevant features I will use fit the model using statsmodels Logit package. This will allow me to see the coefficients of the feature variables and the their corresponding p-values (not possible using Sci-Kit Learn). Note that I add a column of ones to as a constant because the Logit package does not assume a bias term by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 21 Oct 2018</td> <th>  Pseudo R-squ.:     </th>  <td>0.6922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>11:51:12</td>     <th>  Log-Likelihood:    </th> <td> -1114.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>     <td>-3.742e-06</td> <td> 2.26e-06</td> <td>   -1.659</td> <td> 0.097</td> <td>-8.16e-06</td> <td> 6.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>       <td>    0.0960</td> <td>    0.042</td> <td>    2.264</td> <td> 0.024</td> <td>    0.013</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>       <td>    0.0094</td> <td>    0.058</td> <td>    0.161</td> <td> 0.872</td> <td>   -0.104</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>       <td>   -0.0079</td> <td>    0.053</td> <td>   -0.149</td> <td> 0.881</td> <td>   -0.112</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>       <td>    0.6986</td> <td>    0.074</td> <td>    9.454</td> <td> 0.000</td> <td>    0.554</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>       <td>    0.1295</td> <td>    0.067</td> <td>    1.944</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>       <td>   -0.1198</td> <td>    0.074</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.264</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>       <td>   -0.0969</td> <td>    0.067</td> <td>   -1.453</td> <td> 0.146</td> <td>   -0.228</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>       <td>   -0.1739</td> <td>    0.030</td> <td>   -5.711</td> <td> 0.000</td> <td>   -0.234</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>       <td>   -0.2843</td> <td>    0.111</td> <td>   -2.561</td> <td> 0.010</td> <td>   -0.502</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>      <td>   -0.8176</td> <td>    0.097</td> <td>   -8.432</td> <td> 0.000</td> <td>   -1.008</td> <td>   -0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>      <td>   -0.0621</td> <td>    0.081</td> <td>   -0.762</td> <td> 0.446</td> <td>   -0.222</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>      <td>    0.0909</td> <td>    0.087</td> <td>    1.045</td> <td> 0.296</td> <td>   -0.080</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>      <td>   -0.3312</td> <td>    0.082</td> <td>   -4.058</td> <td> 0.000</td> <td>   -0.491</td> <td>   -0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>      <td>   -0.5571</td> <td>    0.062</td> <td>   -8.949</td> <td> 0.000</td> <td>   -0.679</td> <td>   -0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>      <td>   -0.1141</td> <td>    0.086</td> <td>   -1.330</td> <td> 0.183</td> <td>   -0.282</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>      <td>   -0.1908</td> <td>    0.125</td> <td>   -1.526</td> <td> 0.127</td> <td>   -0.436</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>      <td>   -0.0216</td> <td>    0.070</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.159</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>      <td>   -0.0131</td> <td>    0.129</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.266</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>      <td>    0.0963</td> <td>    0.097</td> <td>    0.993</td> <td> 0.321</td> <td>   -0.094</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>      <td>   -0.4582</td> <td>    0.082</td> <td>   -5.607</td> <td> 0.000</td> <td>   -0.618</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>      <td>    0.3898</td> <td>    0.060</td> <td>    6.494</td> <td> 0.000</td> <td>    0.272</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>      <td>    0.6297</td> <td>    0.134</td> <td>    4.707</td> <td> 0.000</td> <td>    0.367</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>      <td>   -0.0951</td> <td>    0.058</td> <td>   -1.629</td> <td> 0.103</td> <td>   -0.209</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>      <td>    0.1289</td> <td>    0.147</td> <td>    0.874</td> <td> 0.382</td> <td>   -0.160</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>      <td>   -0.0761</td> <td>    0.131</td> <td>   -0.582</td> <td> 0.560</td> <td>   -0.332</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>      <td>    0.0195</td> <td>    0.190</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.352</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>      <td>   -0.8188</td> <td>    0.122</td> <td>   -6.686</td> <td> 0.000</td> <td>   -1.059</td> <td>   -0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>      <td>   -0.2937</td> <td>    0.088</td> <td>   -3.332</td> <td> 0.001</td> <td>   -0.467</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.449</td> <td> 0.014</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Constant</th> <td>   -8.3917</td> <td>    0.249</td> <td>  -33.652</td> <td> 0.000</td> <td>   -8.880</td> <td>   -7.903</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.31 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284776\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Sun, 21 Oct 2018   Pseudo R-squ.:                  0.6922\n",
       "Time:                        11:51:12   Log-Likelihood:                -1114.8\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "                                        LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
       "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
       "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
       "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
       "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
       "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
       "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
       "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
       "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
       "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
       "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
       "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
       "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
       "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
       "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
       "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
       "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
       "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
       "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
       "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
       "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
       "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
       "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
       "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
       "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
       "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
       "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
       "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
       "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
       "Amount         0.0009      0.000      2.449      0.014       0.000       0.002\n",
       "Constant      -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "dataset['Constant'] = [1]*len(dataset)\n",
    "X = dataset.iloc[:, dataset.columns != 'Class']\n",
    "y = dataset.iloc[:, dataset.columns == 'Class']\n",
    "del dataset['Constant']\n",
    "\n",
    "logreg = Logit(y, X)\n",
    "results = logreg.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the p-values of the coefficients. Many of our features seem to be insignificantly different from zero. V17 for example has a p-value of 0.757. Removing those features that are insignificant may improve the results of the model. Next I isolate the features with a p-value of less than 0.05.\n",
    "\n",
    "Note a better theoretically superior method is to use forward or backward step selection. However, due to the large number of features and samples, this would be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['V1', 'V4', 'V8', 'V9', 'V10', 'V13', 'V14', 'V20', 'V21', 'V22', 'V27',\n",
       "        'V28', 'Amount'],\n",
       "       dtype='object'), 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = results.pvalues[results.pvalues < 0.05].index\n",
    "cols = cols[cols != 'Constant']\n",
    "cols, len(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 features (shown above) that have p-values of less than 0.05. Rerunning the regression classifier using only these features may improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Used: ['V1', 'V4', 'V8', 'V9', 'V10', 'V13', 'V14', 'V20', 'V21', 'V22', 'V27', 'V28', 'Amount']\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.01\n",
      "{'Recall': 0.8516734693877552, 'Precision': 0.3173983152264528}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.02\n",
      "{'Recall': 0.8333877551020409, 'Precision': 0.5988223289887722}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.03\n",
      "{'Recall': 0.815061224489796, 'Precision': 0.6773725986771526}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.04\n",
      "{'Recall': 0.798734693877551, 'Precision': 0.7088422962417045}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.05\n",
      "{'Recall': 0.7865306122448981, 'Precision': 0.7380011985012496}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.06\n",
      "{'Recall': 0.7742857142857142, 'Precision': 0.7574258722265275}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.07\n",
      "{'Recall': 0.7641632653061224, 'Precision': 0.7743731010368512}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.08\n",
      "{'Recall': 0.7580816326530612, 'Precision': 0.7865673169008507}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.09\n",
      "{'Recall': 0.7519591836734694, 'Precision': 0.795535351197024}\n",
      "----------------------------------------------------------------------------\n",
      "Threshold: 0.1\n",
      "{'Recall': 0.7377551020408164, 'Precision': 0.8017470646873885}\n"
     ]
    }
   ],
   "source": [
    "X = np.array(dataset[cols])\n",
    "y = np.array(dataset.iloc[:, dataset.columns == 'Class']).reshape(len(X),)\n",
    "\n",
    "features = []\n",
    "for c in cols:\n",
    "    features.append(c)\n",
    "print('Features Used: ' + str(features))\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "recall_sig_features = []\n",
    "precision_sig_features = []\n",
    "\n",
    "for threshold in [i/100 for i in range(1, 11)]:\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        logreg = LogisticRegression(solver = 'liblinear')\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "        y_pred_class = [1 if p > threshold else 0 for p in y_pred_proba]\n",
    "        recall_list.append(metrics.recall_score(y_test, y_pred_class))\n",
    "        precision_list.append(metrics.precision_score(y_test, y_pred_class)) \n",
    "    recall_sig_features.append(np.mean(recall_list))\n",
    "    precision_sig_features.append(np.mean(precision_list))\n",
    "    results = {'Recall': np.mean(recall_list), 'Precision': np.mean(precision_list)}\n",
    "    print('----------------------------------------------------------------------------')\n",
    "    print('Threshold: ' + str(threshold))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved. With a precision level of 70%, recall is 80%, an 8 percentage point improvement from the previous model that used all the features. \n",
    "\n",
    "Compared to the baseline case, at a threshold of 0.04 and with feature selection, there is a 19 percentage point increase in recall at the cost of 18 percentage point decrease in precision. This is in contrast to the previous model with no feature selection, where ate a threshold of 0.04, there was a smaller 15 percentage point increase in recall at the cost of a larger 22 percentage point decrease in precision.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "A plot of the recall and precision at each threshold level will help in visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(precision_all_features, recall_all_features, color = 'blue', label = 'All Features')\n",
    "plt.plot(precision_sig_features, recall_sig_features, color = 'red', label = 'Significant Features')\n",
    "plt.title('Precision V Recall as Threshold Increases', fontsize = 14)\n",
    "plt.xlabel('Precision', fontsize = 14)\n",
    "plt.ylabel('Recall', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative slope of the curves is a result of the recall precision trade-off. At all thresholds and precision scores, the model with only the significant features has a higher recall score than the model with all features.\n",
    "\n",
    "This shows that simple feature selection adjustment of threshold probability can markedly improve classification results on imbalanced datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
